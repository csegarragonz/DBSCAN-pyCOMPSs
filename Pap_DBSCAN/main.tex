\documentclass[10pt,journal,compsoc]{IEEEtran}
% * <carlos.segarra@bsc.es> 2017-09-21T09:08:41.072Z:
%
% ^.

\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
\newtheorem{definition}{Definition}
\interdisplaylinepenalty=2500
%\usepackage{algorithmic}
\usepackage{array}
\ifCLASSOPTIONcompsoc 			     	 \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi
\ifCLASSOPTIONcaptionsoff
  \usepackage[nomarkers]{endfloat}
 \let\MYoriglatexcaption\caption
 \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
\fi
\usepackage{url}
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx

%URL
\usepackage{hyperref}
\usepackage{tabularx}

%Plotting
\usepackage{tikz}
%\usepackage{tikz-3dplot} 
\usepackage{pgfplots}

\usepackage{varwidth}% http://ctan.org/pkg/varwidth

%Code snippets
\usepackage{xcolor}
\usepackage{listings}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.98,0.98,0.95}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize\ttfamily,breaklines=true,
    breakatwhitespace=false, 
    xleftmargin=20pt, 
    xrightmargin=20pt,       
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    framextopmargin=50pt,
    frame=bottomline,
    basicstyle=\footnotesize\ttfamily,
    language=Python
}
\lstset{literate=
  {á}{{\'a}}1 {é}{{\'e}}1 {í}{{\'i}}1 {ó}{{\'o}}1 {ú}{{\'u}}1
  {Á}{{\'A}}1 {É}{{\'E}}1 {Í}{{\'I}}1 {Ó}{{\'O}}1 {Ú}{{\'U}}1
  {à}{{\`a}}1 {è}{{\`e}}1 {ì}{{\`i}}1 {ò}{{\`o}}1 {ù}{{\`u}}1
  {À}{{\`A}}1 {È}{{\'E}}1 {Ì}{{\`I}}1 {Ò}{{\`O}}1 {Ù}{{\`U}}1
  {ä}{{\"a}}1 {ë}{{\"e}}1 {ï}{{\"i}}1 {ö}{{\"o}}1 {ü}{{\"u}}1
  {Ä}{{\"A}}1 {Ë}{{\"E}}1 {Ï}{{\"I}}1 {Ö}{{\"O}}1 {Ü}{{\"U}}1
  {â}{{\^a}}1 {ê}{{\^e}}1 {î}{{\^i}}1 {ô}{{\^o}}1 {û}{{\^u}}1
  {Â}{{\^A}}1 {Ê}{{\^E}}1 {Î}{{\^I}}1 {Ô}{{\^O}}1 {Û}{{\^U}}1
  {œ}{{\oe}}1 {Œ}{{\OE}}1 {æ}{{\ae}}1 {Æ}{{\AE}}1 {ß}{{\ss}}1
  {ű}{{\H{u}}}1 {Ű}{{\H{U}}}1 {ő}{{\H{o}}}1 {Ő}{{\H{O}}}1
  {ç}{{\c c}}1 {Ç}{{\c C}}1 {ø}{{\o}}1 {å}{{\r a}}1 {Å}{{\r A}}1
  {€}{{\euro}}1 {£}{{\pounds}}1 {«}{{\guillemotleft}}1
  {»}{{\guillemotright}}1 {ñ}{{\~n}}1 {Ñ}{{\~N}}1 {¿}{{?`}}1
}
\lstset{style=mystyle}
%\pgfplotsset{compat=1.14}

\begin{document}

\title{DBSCAN for PyCOMPSi\\ \LARGE{A distributed approach}}

\author{Carlos~Segarra \\ 
\href{mailto:carlos.segarra@bsc.es}{carlos.segarra@bsc.es}}

\IEEEtitleabstractindextext{%
\begin{abstract}
% Abstract structure:
% - Introduce the importance of the topic and the scope. DONE
% - Describe very briefly what has been done (general description without details). DONE
% - Give a global overview of the achievements. DONE
% - REVIEW: 

The DBSCAN algorithm is one of the most popular clustering techniques used nowadays. However there is a lack of distributed implementations. Additionally, parallel implementations fail when they try to scale to hundreds of cores. The scope of this report is to provide a novel implementation of the algorithm that behaves well in large distributed architectures and with big datasets. Using the PyCOMPSs framework and testing on the Mare Nostrum 4 supercomputer, encouraging results have been obtained reaching a 2467 speedup when run with 4096 cores. Everything whilst keeping the code clean and transparent to the user.
\end{abstract}

\begin{IEEEkeywords}
DBSCAN, Clustering, Machine Learning, Distributed Computing, COMPSs, PyCOMPSs
\end{IEEEkeywords}}
\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

% Introduction structure {
% - Introduce the topic and the scope (give a use case can be helpful). DONE
% - Brief overview of DBSCAN (what is it used for?) DONE
% - Purpose of the article. * DONE
% - Motivation and objective/s. * ??
% - Contribution of the paper. * DONE
% - Define the structure of the article DONE
%   * These points can be in another order.

Big data and data mining is on the daily agenda of nowadays' engineers and having tools to process and analyze this data quickly is of vital importance. Precisely, machine learning, and more precisely, unsupervised learning and clustering are useful for detecting trends and patterns between data without needing the user to previously classify it. The DBSCAN belongs to this family of methods and is useful for detecting non-convex (arbitrarily-shaped) clusters as explained in Section \ref{sec:background}.

This document covers the first implementation of the DBSCAN clustering algorithm within the COMPSs \cite{compss} framework for distributed computing. The main scope of this paper is to accomplish a reasonable speedup when scaling to thousands of cores, being able to process datasets of hundreds of thousands or millions of points in a reasonable amount of time.

Our main contribution is a simple algorithm, completely sequential at first glance, that a standard programmer without much knowledge on concurrency could maintain and develop, programmed in Python (which enhances even more the readability) that scales to thousands of nodes and performs well on big clusters. 

The structure of the paper is the following. Section \ref{sec:related_work} covers related work, other attempts to implementing a parallel version of the DBSCAN in other programming frameworks as well as other clustering algorithms. It briefly covers different benchmarks related to performance and emphasizes the novelty of our proposal. Following along, Section \ref{sec:background} introduces the state of the art of clustering algorithms, describes the Density-Based Spatial Clustering Algorithm the programming model chosen for the implementation and covers the environment where tests have been performed. Section \ref{sec:implementation} describes the algorithm developed to fulfill the scope of the paper, the dependency graph drawn by it and covers possible equivalence issues faced when reinterpreting a sequential algorithm as a parallel one. Section \ref{sec:performance} summarizes all the tests performed with synthetic and real data, presents the results and gives a critical review to the algorithm using post-mortem analysis tools and tracing. Lastly Section \ref{sec:conclusion} gathers the conclusions obtained and proposes further developments that did not fit our initial scope.
% }

\section{Related Work} \label{sec:related_work}

Quickly after the algorithm was first presented (1996) the necessity to feed the clustering algorithm with big datasets arose. First approaches of DBSCAN parallelization only dealt with the region query issue which is indeed where more calculus power is invested. For instance, \cite{related_1} presents a skeleton-structured program where slaves are responsible for answering the regions query performed by the master.

Latter developments started including smart data partitioning. \cite{related_2} proposes what has become a standard procedure for distributed implementations of the DBSCAN. Initially data is efficiently divided, secondly a series of partial clusterings are performed and final results are merged by the master. Most implementations differ on both the partition and the merging. \cite{related_2} divides data using a low-precision hierarchical clustering algorithm and merges the data using a graph connectivity technique similar to the one used in this paper. Graph connectivity techniques are also used in \cite{related_4}. Yaobin HE et. al \cite{related_3} propose a novel partitioning method based on computational costs predictions and merge results using a 4 step MapReduce strategy. 

Latest developments focus on flexibility and scalability to high feature datasets. A. Lulli \cite{related_5} presents a Spark-based algorithm that does not require euclidean metric and that works with arbitrary data.

The novelty of our proposal lies that in all the previously mentioned papers, the code might be quite difficult to understand and specially to mantain for a user. Taking advantage of the COMPSs framework for distributed programming, the final code produced is highly-readable.
% Add related works (publications) with the same or similar objective (even for another clustering algorithms or frameworks). DONE

% Don't forget to point out the differences between other proposals and the one presented in the paper. DONE 
% I would try to concentrate the main difference/novelty as the last phrase of this section to remark what makes this work innovative compared to others. DONE




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%     BACKGROUND       %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Background} \label{sec:background}

% JC: This section can be used to provide the necessary background for understanding the following sections.
% JC: Here, DBSCAN can be defined formally, as well as PyCOMPSs.

This section provides the necessary information to firstly understand the implementation proposed and secondly to contextualize the tests performed.

\subsection{The algorithm: DBSCAN} \label{the_algorithm}

% Formal definition of DBSCAN

The \textbf{D}ensity-\textbf{B}ased \textbf{S}patial \textbf{C}lustering \textbf{A}lgorithm with \textbf{N}oise (\textbf{DBSCAN}) is a clustering algorithm based on point density. It was proposed in 1996 \cite{ReferencePaper} and has become one of the reference techniques for non-supervised learning. To take a first dive into the implementation, a few previous definitions are mandatory.

\begin{definition}
The $\varepsilon$\textbf{-neighborhood} of a point $p$ from a dataset $D$ and given a distance $d$ is
$$N_{\varepsilon}(p) = \lbrace q \in D: d(p,q) < \varepsilon \rbrace$$ 
\end{definition}

The general approach partitions the set of points in three subsets:
\begin{definition}
\begin{itemize}
\item A point $p$ is said to be \textbf{\textit{core point}} if it has over \textit{minPoints} neighbors within its $\varepsilon$-neighborhood.
\item A point $p$ is said to be \textbf{\textit{reachable}} if it lies within a $\varepsilon$-neighborhood of a core point in spite of not being one.
\item A point $p$ is said to be \textbf{\textit{noise}} if it does not fulfil any of the previous definitions.
\end{itemize}
\end{definition}

\begin{definition}
A \textbf{Cluster} $C$ is a subset of a dataset $D$ such that for all $p \in C$:
\begin{itemize}
\item[(i)] $p$ is a core-point or reachable
\item[(ii)] $\forall p,q \in C, \exists p_1, \ldots,  p_r$ fulfilling (i) such that $p \in N_{\varepsilon}(p_1)$, $p_i \in N_{\varepsilon}(p_{i+1})$ and $p_r \in N_{\varepsilon}(q)$ where only, maybe $p$ and/or $q$ are reachable
\end{itemize}
\end{definition}

\begin{definition} \label{def_distance}
The \textbf{distance between sets} $A$ and $B$ given a metric d is
$$d(A,B)=\min \lbrace d(a,b): a \in A, b \in B \rbrace$$
\end{definition}

\subsubsection{Other Clustering Algorithms} \label{sec:other-clust-alg}

% JC: I like this subsubsection, since it focuses on the difference with other clustering algorithms rather than being a related work, and helps to understand DBSCAN.
Recently a lot of research has been dedicated to improving clustering algorithms, resulting in the current classification in four different categories: hierarchical clustering, centroid-based clustering, distribution-based clustering and density based clustering.

In comparison to its main competitor the \textbf{k-means} algorithm \cite{kmeans}, DBSCAN is robust to outliers, it does not require an initial guess of the number of clusters and it is able to detect non-convex clusters as exposed in Figure \ref{fig_comp_clust}. To obtain the Figure the dataset has been generated using the \textit{Sklearn} package for datasets (\href{http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html}{\texttt{make\_moons}}), the DBSCAN algorithm is the one presented in this document and the k-means is a personal implementation available on the \href{https://github.com/csegarragonz/DBSCAN-pyCOMPSs}{project's GitHub repository}.

\begin{figure*}[!t]
\centering
\subfloat[Clusters found by DBSCAN.]{\includegraphics[width=3.5in]{img/moonsDBSCAN.png}%
\label{fig_first_case}}
\hfil
\subfloat[Clusters found by k-means.]{\includegraphics[width=3.5in]{img/moonsKMEANS.png}%
\label{fig_second_case}}
\caption{Comparison of the clusters provided by DBSCAN and k-means over set containing non-convex clusters.}
\label{fig_comp_clust}
\end{figure*}

\subsubsection{S.o.A for sequential and distributed implementations}

% JC: I also like this subsection. 

There are a variety of implementations of the DBSCAN raging from the more naive ones to more complex ones. As for comparison, a naive implementation following the exact guidelines of \cite{ReferencePaper} can be found \href{https://github.com/csegarragonz/DBSCAN-pyCOMPSs/blob/master/ext_versions/DBSCAN_Seq.py}{here}. One of the most extended and used versions programmed in \texttt{Python} is the one by \href{http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html}{Sklearn}.

When it comes to distributed implementations of the algorithm, most of them can be summarized in applying an efficient DBSCAN to a chunk of the dataset and using some sort of synchronization or MapReduce. Otherwise, the DBSCAN for PyCOMPSs reformulates the algorithm trying to adapt it to distributed architectures making sure it is still equivalent to the original implementation.

\subsection{The framework: COMPSs} \label{subec:compss_framework}

% COMPSs/PyCOMPSs definition

The \href{https://www.bsc.es/research-and-development/software-and-apps/software-list/comp-superscalar}{COMPSs} framework\cite{compss} is a programming model designed to ease the development of applications for distributed architectures. The user programs a sequential code and defines the so-called \texttt{tasks}. The COMPSs runtime infers the dependencies and schedules the executions basing on the resources available. The model is developed in Java but has bindings for both \texttt{C} and \texttt{Python}. \texttt{PyCOMPSs}\cite{pycompss} is the model chosen to develop the application. A master orchestrates a series of workes with a certain number of threads that are responsible of running tasks. In order to mark a function as a task for the scheduler to take it into account, a small decorator must be added. \texttt{COMPSs} is complemented by a set of tools for facilitating the monitoring and the post-mortem performance analysis (see Section \ref{performance}).

% \begin{figure}
% \animategraphics[autoplay,controls,loop,scale=1]{3}{img/anim-}{0.png}{4.png}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%     PROPOSAL       %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementation Proposal} \label{sec:implementation}

% JC: Describe all implementation details of the proposal. How has DBSCAN been parallelised, parameters, etc.
% JC: Details about how it has been evaluated should be in the evaluation section.

The algorithm implemented is a reinterpretation of the one exposed in \cite{ReferencePaper}, it is still though completely equivalent. The implementation can be found in \href{https://github.com/csegarragonz/DBSCAN-pyCOMPSs}{this GitHub repository}. The main algorithm would be structured as in Algorithm \ref{alg:main} (note that all the methods stated are later expanded and explained). It takes as an input the two required parameters: $\varepsilon$ and \texttt{min\_points} as well as the path to a distirbuted dataset stored in some sort of database\footnote{For the speciphic implementation given, the structure assumed is that of the GPFS in \ref{subsec:infrastructure} and points are assumed to be divided in a grid. Otherwise a linear classificlation should be incoporated in the pre-processing.}. It outputs the same dataset labeled with the cluster id they belong to.

\begin{algorithm}
  \caption{Main method for the DBSCAN algorithm. \label{alg:main}}
  \begin{algorithmic}[1]
    \Function{DBSCAN}{$data\_file$, $epsilon$, $min\_points$, $frag\_size$}
    	\State $norm\_data \gets \text{normalize\_data}(data\_file)$
		\State $frag\_data \gets \text{partition\_space}(norm\_data, frag\_size)$
        \ForAll{$square$ \textbf{in} $frag\_data$}
            \State \begin{varwidth}[t]{\linewidth}
            	$core\_points \gets \text{partial\_scan}(square, epsilon, $ \par
                	\hskip\algorithmicindent $min\_points, frag\_data)$ \par
                $clusters[square] \gets \text{merge\_cluster}(core\_points,$ \par 
                \hskip\algorithmicindent $epsilon)$
            \end{varwidth} 
     	\EndFor
        \State $clusters = \text{compss\_wait\_on}(clusters)$
        \State $clusters\_links = sync\_clusters(clusters)$
        \State $clusters = \text{update}(clusters, clusters\_links)$
        \State $\textbf{return} \text{expand\_clusters}(clusters)$
    \EndFunction
  \end{algorithmic}
\end{algorithm}
where the \texttt{update} method finds the connected components of a graph given its adjacency matrix (using an MF-Set) and merges the corresponding clusters.

\subsection{Step-by-Step analysis} \label{step_by_step}

\subsubsection{Data Pre-Processing} \label{data_pre_process}
Firstly, data is normalized by dividing each axis by the maximum value along it. This is done to prevent different factors from being really unbalanced and to ease the input parameters choice.

\begin{algorithm}
  \caption{Given a dataset, divide each dimension by its maximum value \label{alg:normData}}
  \begin{algorithmic}[1]
    \Function{normalize\_data}{$data\_file$}
		\State $data \gets \text{load}(data\_file)$
        \ForAll{dimensions of $data$}
       		\State Divide by th maximum along its axis
     	\EndFor
        \State \textbf{return} \text{save}($data$)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\subsubsection{Adaptative Partitioning} \label{partition_space} 
Secondly, the dataset is partitioned depending on the point density. Initially the three partitioning points among each axis are the minimum value, the arithmetic mean and the maximum value, these values are stored in \texttt{fragVec}. Iteratively, an additional point is added in the middle of the interval containing the higher amount of points, this is done \texttt{fragSize} times along each dimension. By performing this partition, excessive unbalancing of points per square can be avoided. Once this step is finished, the dataset is stored in a dictionary where the key is a unique identifier of each spatial region and the value is a list containing all the points in that region.

\begin{algorithm}
  \caption{Return a space partition basing on point density \label{alg:partitionData}}
  \begin{algorithmic}[1]
    \Function{partition\_space}{$dataset, frag\_size, epsilon$}
		\State Set $dimension, fragVec$ 
        
        \ForAll{dimensions of $data$}
        
        	\For{$i \gets 1, frag\_size$}
            	\ForAll{$p$ \textbf{in} $dataset$}
                
       				\State Find most dense interval.
                \EndFor
                \State Divide that interval in two.
        	\EndFor
     	\EndFor
        \State \textbf{return} \text{save}($data$)
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Due to the theoretical possible large amount of points per cluster, an extra scale parameter is added (\texttt{num\_parts}), each worker will be assigned to a \texttt{num\_parts} part of each square resulting of the partition.

\subsubsection{Initial Neighbor Retrieval} \label{neigh_retr} 
Once the dataset is correctly split, each worker performs a core point retrieval, please note \emph{core points} retrieval. To optimize this query, each worker knows which square he has been assigned to and consequently only looks for possible neighbors in all the adjacent squares within $\varepsilon$ distance. The query structure is the one presented in Figure \ref{fig:query}. All the points from the squares reachable with $\varepsilon$ distance from the square where the point is are considered as possible neighbors. If a point has over \texttt{min\_points} neighbors then it is considered a core point.

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{img/dibuixEps.png}
\caption{In darker red the square where the point belongs, in lighter red the squares where the algorithm will look for possible neighbors.}
\label{fig:query}
\end{figure}

With this core points, a first cluster proposal is made (a cluster is a subset of the points assigned to that worker, however each worker has access to its neighbor squares' points to determine whether a point neighborhood contains enough people). In the example of Figure \ref{fig_clustering}, \texttt{num\_parts} has been fixed to 1 so that each square is assigned to one and only one worker, otherwise intermediate results would be difficult to visually understand.
\begin{algorithm}
  \caption{Looks for all the core points in a certain square\label{alg:partialScan}}
  \begin{algorithmic}[1]
  	\State @task()
            	\Function{partial\_scan}{$square, epsilon, min\_points$}  
            \State \begin{varwidth}[t]{\linewidth}
                	\hskip\algorithmicindent $frag\_data$) 
            \end{varwidth} 
		\State $point\_set\_real \gets \text{get\_data}(square)$ 
        \State $point\_set \gets \text{get\_data(get\_neighbors}(square))$
        \ForAll{$point$ \textbf{in} $point\_set\_real$}
        	\ForAll{$pos\_neigh$ \textbf{in} $point\_set$}
            	\If{distance($point, pos\_neigh$) $\leq$ $epsilon$}
                	\State $point$.neighbors += 1
                \EndIf
      		\EndFor
        	\If{$point$.neighbors $\geq$ $min\_points$}
            	\State $core\_points\text{.add}(point)$
          	\EndIf
     	\EndFor
        \State \textbf{returns} $core\_points$
    \EndFunction
  \end{algorithmic}
\end{algorithm}
\begin{figure*}[!t]
\centering
\subfloat[Dataset with adaptative partitioning.]{\includegraphics[width=3.5in]{img/anim-1.png}%
\label{fig_f_case}}
%\hfil
\subfloat[Initial cluster proposal.]{\includegraphics[width=3.5in]{img/anim-2.png}%
\label{fig_s_case}}
\hfill 
\subfloat[Clusters after synchronization.]{\includegraphics[width=3.5in]{img/anim-3.png}%
\label{fig_t_case}}
\hfil
\subfloat[Final output.]{\includegraphics[width=3.5in]{img/anim-4.png}%
\label{fig_se_case}}
\caption{Plotting of the current result at each step of the algorithm.}
\label{fig_clustering}
\end{figure*}

\subsubsection{Cluster Synchronization} \label{cluster_sync}
As a result of \ref{neigh_retr}, a huge list of clusters is obtained. To sync the corresponding results the clusters are reinterpreted as nodes in a graph and an adjacency matrix is built. The $(i,j)$-th element in the matrix will be true if and only if the distance between the $i$-th and the $j$-th cluster is lower than $\varepsilon$, see Definition \ref{def_distance}. Equivalently, each synced cluster will correspond to a connected component of the graph determined by the adjacency matrix (here a synced cluster refers to a cluster post-synchronization). Using a merge-find set, this dependencies are quickly sorted and the connected components easily found.
Since the initial number of clusters found depends on the amount of partition points chosen and on the data distribution, an extra scale parameter is introduced to prevent the algorithm of scheduling too many tasks (one per comparison). This scale parameter, $numComp$ determines how many cluster vs cluster comparisons a single task performs.

\begin{algorithm}
  \caption{Returns an adjacency matrix \label{alg:syncClusters}}
  \begin{algorithmic}[1]
    \Function{sync\_clusters}{$clusters$}
		\ForAll{$clust1$ \textbf{in} $clusters$}
        	\ForAll{$clust2$ \textbf{in} $clusters$}
            	\State @task()
            	\If{distance($clust1, clust2$) $\leq$ $epsilon$}
            		\State $adjMatrix$[$clust1$.num, $clust2$.num] $\gets$ 1
                \EndIf
           	\EndFor
     	\EndFor
        \State \textbf{return} $adjMatrix$
    \EndFunction
  \end{algorithmic}
\end{algorithm}


\subsubsection{Cluster Expansion} \label{cluster_exp} 
Once all the information is updated, the current result is a set of clusters of core-points. A second neighbor retrieval is performed, assigning a part of each cluster to a worker, to check for reachable points and add them non-deterministically. Finally results are exported to a text file and, if selected, clusters are plotted and the plot saved to file.

\begin{algorithm}
  \caption{Expands all clusters from the list \label{alg:expandClusters}}
  \begin{algorithmic}[1]
 	\Function{expand\_clusters}{$clusters$}
		\ForAll{$clust$ \textbf{in} $clusters$}
			\State $tmp\_points$ $\gets$ get\_neighbors($clust$.square)
            \State $poss\_neigh$ $\gets$ $tmp\_points$ \textbf{not in} $clust$.points
       		\ForAll{$point$ \textbf{in} poss\_neigh}
            	\If{distance($point, clust \leq epsilon$)}
                	\State $clust$.add($point$)
               	\EndIf
          	\EndFor
        \EndFor
    \EndFunction
  \end{algorithmic}
\end{algorithm}


The whole process is summarized in Figure \ref{fig_clustering}.

\subsection{Dependency Graph}
The COMPSs framework introduced in Section \ref{compss_framework} incorporates a dependencies graph generating script to improve the comprehension on the application behavior. 

In Figure \ref{dep_graf} two main bottlenecks in the execution can be observed (besides the last synchronization which is unavoidable). Relating to the step by step description, the first one corresponds to the initial neighbor retrieval and the first cluster proposal. One might think that this step does not need a synchronization point since the cluster vs cluster comparison does not require all the clusters to be found just the two required to be compared. Even though this might sound true in theory, the fact that the points from a square need to be sub-partitioned again for size issues and the fact that in order to build the adjacency matrix one must know the number of nodes his graph will have make removing this synchronization point nearly impossible (at least with this implementation). However there are as well possible workarounds. For instance, a \texttt{MAX\_CLUSTS} variable could be defined and all the sizes prefixed. This goes however against one of the DBSCAN's main principle, that no information about the number of clusters is required to begin the execution. Further development on improvement proposals is exposed in Section \ref{further_development}. The second bottleneck comes after the initial cluster proposal is synced. Once we know for each cluster which clusters it can be merged too, it is mandatory to wait for all the information processed since otherwise a lot of time could be lost in moving points from a cluster to another. For instance, if cluster 1 must be merged with cluster 2 and cluster 2 with clusters 1 and 3, not waiting for all the adjacency matrix to be computed would represent copying all the data twice with the corresponding loss in performance.

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{img/complete_graph.pdf}
\caption{Dependency graph for a small execution.}
\label{dep_graf}
\end{figure}


\subsection{About equivalence}

Basing on the definitions given in Section \ref{the_algorithm} and the implementation exposed in Section \ref{step_by_step}. It is pretty clear that the algorithm performs completely equivalently to the one stated in \cite{ReferencePaper}. The neighbor retrieval is the same as in the reference implementation. When syncing two different situations may appear.
\begin{itemize}
\item \textbf{Clusters in different squares.} This is one of the most common situations. A natural cluster divided by the space partition performed at \ref{partition_space}. If both clusters should really be merged, then at least one point from each cluster must be at a distance lower than $\varepsilon$ and, if so, the syncing process will merge them. This situation is illustrated in Figure \ref{test_case1}. Bear in mind though that after \ref{neigh_retr} only clusters of core points are found.  What if the linking point between two clusters is not a core point?
\begin{figure}[!h]
\centering
\includegraphics[width=2.5in]{img/test_case1.png}
\caption{A cluster divided by the chunking process.}
\label{test_case1}
\end{figure}
\item \textbf{Clusters that share a non-core point.} According to the reference paper, if two point-dense regions lie within a $\varepsilon$-neighborhood of the same point, even if this point might only have two neighbors, then this must be non-deterministically assigned to one of the two clusters. This does not represent a problem to the initial algorithm since its sequentiality prevents it from needing to take a decision. In \ref{cluster_exp} at the Cluster Expansion Stage the same point will be assigned to both clusters but only assigned to the first of them in the cluster list. This situation is illustrated in Figure \ref{test_case2}.
\begin{figure}[!h]
\centering
\includegraphics[width=2.5in]{img/test_case2.png}
\caption{Cluster linked by a non-core point.}
\label{test_case2}
\end{figure}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%     EVALUATION       %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Evaluation} \label{sec:performance}

% JC: Add a section entry phrase for the evaluation. DONE
% JC: Subsection with the infrastructure (hardware description, software versions, etc.) DONE
% JC: Subsection with the configurations evaluated, as well as the metrics considered for the evaluation.
% JC: Results and explanations of them. If more of one evaluation is performed, divide them into subsections (e.g. performance achieved, scalability experiments, etc.) DONE

This section covers the evaluation of the implementation presented. The first subsection \ref{subsec:infrastructure} covers how was the algorithm implemented and tested. The second subsection \ref{subsec:metrics} gathers all the data collected from the different experiments performed. The last subsection \ref{subsec:critical} provides a critical review on the results, what was expected and what has been achieved.

\subsection{Infrastructure} \label{subsec:infrastructure}

As mentioned before, the algorithm has been implemented fully in Python within the PyCOMPSs framework. COMPSs version is the 2.1 and Python's is the 2.7.13 since COMPSs does not support Python 3.X. To run the program hence, the user must have COMPSs installed, the corresponding Python version and the numeric python library (numpy). The algorithm can be ran both locally and in a cluster, in the project's GitHub repository there is a shell script (\texttt{run.sh}) that contains methods to invoke the DBSCAN both as local host (through \texttt{runcompss} command) and to a queuing system (through \texttt{enqueue\_compss} command). 

The experiments were ran on a cluster located in Barcelona. The \href{https://www.bsc.es/user-support/mn4.php#systemoverview}{Mare Nostrum 4} (MN4) is the fourth generetion of supercomputers located in the \href{https://www.bsc.es}{Barcelona Supercomputing Center - Centro Nacional de Supercomputacion (BSC-CNS)} its current Linpack Rmax Performance is 6.2272 Petaflops and it has been running since early July 2017. It is equipped with 48 racks with 3456 nodes. Each node has two Intel Xeon Platinum chips with 24 processors per unit. COMPSs through its queuing script for clusters, \texttt{enqueue\_compss}, requires each node on exclusivity. Therefore from now on, whenever a node is mentioned it must be considered that no other program is going to be running there besides our execution. Additionally the queuing script incorporates a \texttt{cpus\_per\_node} flag that determines the number of processors running in each node.

\subsection{Runtime Metrics} \label{subsec:metrics}

To evaluate the algorithm's performance for different dataset sizes (all three dimensional but with different number of points) the execution time has been measured. This duration is computed inside the python method using the built-in tools and printed through the standard output. For the same dataset, a batch of executions with different number of workers was performed to test the algorithm's scalability. The input parameters vary between different data sizes but never inside the same batch. All the executions were ran in MN4 (\ref{the_machine}) with Scratch file system. Results are summarized in Table \ref{performance_table}

\begin{table*}[!t]
\centering
\caption{Performance measured on MN4 without tracing and 16 CPUs per node.}
\label{performance_table}
\begin{tabular*}{\textwidth}{c|cc|cc|cc|cc|cc}
 \textbf{SCRATCH}   & \textbf{NP=16} & \textbf{10k} & \textbf{NP = 16} & \textbf{20k} & \textbf{NP = 32} & \textbf{50k} & \textbf{NP = 32} & \textbf{100k} &  \textbf{NP=32} & \textbf{ 200k}\\ \cline{2-11} 
\textbf{Workers}     & \textbf{Time (sec)}   & \textbf{SUp}   & \textbf{Time (sec)}     & \textbf{SUp}   & \textbf{Time (sec)}     & \textbf{SUp}   & \textbf{Time (sec)}    & \textbf{SUp}    & \textbf{Time (sec)}   & \textbf{SUp} \\ \cline{1-11}
\textbf{1}           & 36.12        & 1.00              & 125.03          & 1.00              & 820.14           & 1.00              & 3160.46          & 1.00  &   15030.99 & 1.00          \\
\textbf{2}           & 22.47          & 1.63           & 71.66            & 1.74           & 423.36           & 1.93           & 1623.92          & 1.94 & 7659.83 & 1.96        \\
\textbf{3}           & 18.05          & 2.02           & 55.04           & 2.27           & 308.014          & 2.62           & 1165.10         & 2.71 & 4547.23 & 3.3           \\
\textbf{4}           & 15.91         & 2.25           & 45.60           & 2.74           & 249.59           & 3.28           & 910.30            & 3.47   & 3634.97 & 4.13        \\
\textbf{5}           & 15.01          & 2.40           & 42.49           & 2.94           & 227.93           & 3.59           & 777.19          & 4.06  & 3134.47 & 4.79         \\
\textbf{6}           & 14.55          & 2.48           & 40.23            & 3.16           & 196.69           & 4.16           & 703.90            & 4.48 & 2736.65 & 5.49           \\
\textbf{7}           & 14.3           & 2.52           & 36.36           & 3.43           & 184.00              & 4.45           & 620.53          & 5.09 & 2557.32 & 5.87           \\
\textbf{8}           & 13.64         & 2.64           & 33.61           & 3.72           & 171.00              & 4.79           & 609.21          & 5.18 & 2376.56 & 6.32           \\
\textbf{16}          & 13.45          & 2.68           & 29.02           & 4.30           & 143.00              & 5.73           & 433.12          & 7.29 & 1565.82 & 9.59           \\ \cline{1-11}
\textbf{Seq} & 4690.80         &                & ---              &                & ---              &                & ---              & & --- &                
\end{tabular*}
\end{table*}

\begin{figure}[h!]
\centering	
\begin{tikzpicture}
\begin{axis}[
height=2in,
width=3.5in,
    scaled x ticks = false,
    xlabel = {\# Workers},
    grid = major,
    ylabel = {Speed Up},
    legend entries={10k, 20k, 50k, 100k, 200k}, legend style={at={(0.05,-0.25)},anchor=north west, draw=none, legend columns=-1}
    %xtick={0.028503, 0.0427553,0.0570},
    %xticklabels={$l-2\lambda$, $l-\lambda$,$l$},
    %ytick={0,1,2},
    %yticklabels={0,$|E_{0i}|$,$2|E_{0i}|$},
]
\addplot[mark=o, thick, blue] table {data/10k.dat};
\addplot[mark=o, thick, orange] table {data/20k.dat};
\addplot[mark=o, thick, red] table {data/50k.dat};
\addplot[mark=o, thick, green] table {data/100k.dat};
\addplot[mark=o, thick, purple] table {data/200k.dat};
%Here the blue parabloa is defined
\end{axis}
\end{tikzpicture}
\caption{Speed Up as a function of the number of workers for different datasets.}
\label{speed_up}
\end{figure}

In figure \ref{speed_up} there is a speed up vs number of workers plot. To understand why it increases as the number of points increases check section \ref{q_coi_passa}. Additionally, a runtime vs number of workers plot can be found at \ref{rt_plot}.

\begin{figure}[h!]
\centering	
\begin{tikzpicture}
\begin{axis}[
height=2in,
width=3.5in,
    scaled x ticks = false,
    xlabel = {\# Workers},
    grid = major,
    ylabel = {Execution Time (sec)},
    legend entries={10k, 20k, 50k, 100k, 200k}, legend style={at={(0.05,-0.25)},anchor=north west, draw=none, legend columns=-1}
    %xtick={0.028503, 0.0427553,0.0570},
    %xticklabels={$l-2\lambda$, $l-\lambda$,$l$},
    %ytick={0,1,2},
    %yticklabels={0,$|E_{0i}|$,$2|E_{0i}|$},
]
\addplot[mark=o, thick, blue] table {data/10kRT.dat};
\addplot[mark=o, thick, orange] table {data/20kRT.dat};
\addplot[mark=o, thick, red] table {data/50kRT.dat};
\addplot[mark=o, thick, green] table {data/100kRT.dat};
\addplot[mark=o, thick, purple] table {data/200kRT.dat};
%Here the blue parabloa is defined
\end{axis}
\end{tikzpicture}
\caption{Execution time as a function of the number of workers for different datasets.}
\label{rt_plot}
\end{figure}

\subsection{Critical Analysis} \label{subsec:critical}

\subsubsection{Dependence on the scale parameters:}

From the explanation in Section \ref{the_algorithm} it is easy to see that the paradigm has shifted from a two parameter algorithm to a four one. In the one hand this enables, when run by someone familiar to it, the algorithm to adapt better to mutable resources and data loads. In the other hand this also increases the variability of the performance, may be a drawback at some points and makes the implementation not 100\% structure-unaware.

Emphasizing on this two extra parameters, one of them has not got a clear impact on the runtime metrics. The number of comparisons executed by a worker, \texttt{numComp} will determine a certain type of task's length but won't increase their variability. However, \texttt{numParts} chunks the data again having a direct effect on the number of clusters initially found and consequently on the number of tasks performed at the syncing stage. In addition to that, it reduces the average task duration and at the same time makes tasks less homogeneous. This process causes bigger executions' (more points and assigned nodes) behavior more volatile. In spite of that, this scale parameter is necessary to ensure the algorithm's scalability, otherwise it would depend entirely on the data distribution.

\begin{figure}[!h]
\centering
\includegraphics[width=3.5in]{img/traca_compacte.png}
\caption{Tracing with 10k points and $\texttt{numParts}=16$.}
\label{traca_bona}
\end{figure}

To dig in this sections where the algorithm is not performing as expected, or at least not as desired, we undergo a performance analysis using tracing (\href{https://tools.bsc.es/paraver}{Paraver}). In Figure \ref{traca_bona} there is an example of a small execution with  $\texttt{numParts}=16$ (relatively small value of the parameter) and 10000 points. Consequently with the above lines a homogeneous trace is obtained. In Figure \ref{traca_caca} the opposite behavior is exposed. For this second execution the number of points was around 20000 and $\texttt{numParts}=32$. It is easy to see that tasks durations become much more unstable (green flags indicate a task's beginning and ending) and they are not so easily scheduled by the COMPSs runtime.

\begin{figure}[!h]
\centering
\includegraphics[width=3.5in]{img/traca_mes_caca.png}
\caption{Tracing with 20k points and $\texttt{numParts}=32$.}
\label{traca_caca}
\end{figure}

\subsubsection{On memory, CPUs per node and scalability:} \label{q_coi_passa}

Another issue faced when testing the algorithm is the memory vs cpu per node paradigm. Initially each node has 2GB of RAM, however if the user requests less CPUs per node and exclusivity each worker will virtually have more random access memory since the whole batch is divided between the running threads. When the dataset tends to get bigger and bigger, the memory requested increases as well. Hence the performance at some points might be better with less \texttt{cpus\_per\_node} in spite of the time lost with inter-node communication. To dig deeper into this situation, another set of tests is performed. Results are summarized in Table \ref{cpusComparison}. The \texttt{cpus\_per\_node} values to be compared are 16 and 48. 48 is 16 times 3, therefore to perform a fair comparison the number of workers assigned to the 16 \texttt{cpus\_per\_node} test must be three times the ones assigned to the 48 \texttt{cpus\_per\_node} one. Surprisingly enough given the reasoning above, results do not show a clear dominance of 16 CPUs per node tests over 48 CPUs ones with small datasets. As the size of the input grows and as more calculations are required, the performance is the predicted and 16 \texttt{cpus\_per\_node} tests are quicker.

\subsubsection{On time complexity}

Figures \ref{timeComplexity1} and \ref{timeComplexity2} plot the evolution of the execution time as a function of the size of the input, i.e the number of points, for a fixed number of working nodes and two different \texttt{cpus\_per\_node} parameters, 16 and 48 respectively. \ref{q_coi_passa} shows that there is no evident difference between the two options, as a consequence both Figures look really similar. The main conclusion one can deduce is that, the higher the number of workers (or CPUs in general) the least the application behaves quadratically as its naive sequential implementation and the better PyCOMPSs performs.

\subsubsection{Testing with real data and comparing with S.o.A implementations}

All the previous tests have been performed with synthetic data. The application must as well prove to be useful in real-life applications and when compared to other implementations. A real case use is the one presented in \cite{toolsDBSCAN} where clustering is applied to an app trace to "outline the different trends of the CPU computation areas of parallel applications". Within the same paper, an implementation of the DBSCAN is proposed and thus we will be able to compare both performances. The trace used is one corresponding to:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%       CONCLUSIONS     %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusion} \label{sec:conclusion}

% Conclusions structure:
% - Describe the achievements of the work (DBSCAN parallelization, performance results, etc.)
% - Considerations (if any).
% - Future work.
%
% JC: There shouldn't be tables of results nor graphs in this section. They should be in the evaluation section.
% JC: I wouldn't divide it in subsections.

Following along the reasoning from \ref{subsec:critical}, the algorithm is really sensitive to changes in the input parameters. Thus this parameters should somehow depend on the resources available, however that would make the algorithm not structure-unaware. This might be one of the reasons why scalability and speed up is not exactly as desired since all the tests for the same dataset are performed under the same input parameters (to be able to compare them). Therefore the measured speedup might be worse in comparison to if we were to run the algorithm manually test by test. 

\begin{table*}[!h]
\centering
\caption{Execution time comparison with the same number of CPUs but different number of workers and different CPUs per node. Time measured in seconds.}
\label{cpusComparison}
\begin{tabular*}{.95\textwidth}{c|cc|cc|cc|cc|cc}
\textbf{Comparison CPUS per Node} & \multicolumn{2}{c|}{\textbf{10k}} & \multicolumn{2}{c|}{\textbf{20k}} & \multicolumn{2}{c|}{\textbf{50k}} & \multicolumn{2}{c|}{\textbf{100k}} &
\multicolumn{2}{c}{\textbf{200k}}
\\ \hline
\textbf{\#Workers - 16 (48)}      & \textbf{16}     & \textbf{48}     & \textbf{16}     & \textbf{48}     & \textbf{16}     & \textbf{48}     & \textbf{16}      & \textbf{48}   & \textbf{16}     & \textbf{48}  \\ \hline
\textbf{3 (1)}                    & 23.44           & 21.81           & 68.22           & 63.6            & 375.47          & 366.49          & 1326.732         & 1322.73  & 5308.99 & 5501.241035    \\
\textbf{6 (2)}                    & 16.74           & 16.94           & 46.15           & 44.62           & 233.71          & 230.68          & 790.97           & 785.68   & 3205.49 & 3412.54       \\
\textbf{9 (3)}                    & 16.609          & 16.42           & 39.71           & 37.77           & 190.39          & 186.69          & 623.78           & 627.52   & 2427.87 & 2451.26       \\
\textbf{12 (4)}                   & 17.78           & 15.74           & 34.43           & 36.01           & 167.83          & 173.65          & 555.06           & 544.66    & 2039.61 & 2012.89      \\
\textbf{15 (5)}                   & 17.23           & 16.92           & 33.71           & 33.177          & 163.87          & 158.08          & 487.01           & 480.03     & 1707.57 & 1842.83     \\
\textbf{18 (6)}                   & 16.76           & 15.63           & 33.166          & 33.84           & 156.11          & 149.75          & 461.73           & 446.91   & 1486.20 & 1708.76      
\end{tabular*}
\end{table*}

\begin{figure}[t]
\centering
\begin{tikzpicture}
    \begin{axis}
        [
        ,width=8cm
        ,xlabel=\# Points
        ,ylabel=Time elapsed (sec)
        ,xtick=data,
       %,xtick={0,1,...,3}
        ,xticklabels={10k,20k,50k,100k, 200k}
        ,legend entries={3, 6, 9, 12, 15, 18}, legend style={at={(0.05,-0.25)},anchor=north west, draw=none, legend columns=-1}
        ]
        \addplot+[sharp plot] coordinates
        {(0,23.44) (1,68.22) (2,375.47) (3,1326.73) (4, 5308.99)};
        \addplot+[sharp plot, red] coordinates
        {(0,16.74) (1,46.15) (2,233.71) (3,790.97) (4, 3205.49)};
        \addplot+[sharp plot, green] coordinates
        {(0,16.609) (1,39.71) (2,190.39) (3,623.78) (4, 2427.87)};
        \addplot+[sharp plot, orange] coordinates
        {(0,17.78) (1,34.43) (2,167.83) (3,555.06) (4, 2039.61)};
        \addplot+[sharp plot, yellow] coordinates
        {(0,17.23) (1,33.43) (2,163.87) (3,487.01) (4, 1707.57)};
        \addplot+[sharp plot, purple] coordinates
        {(0,16.76) (1,33.16) (2,156.11) (3, 461.73) (4, 1486.2)};
    \end{axis}
\end{tikzpicture}
\caption{Evolution of time elapsed with a fixed number of workers. 16 CPUs per node. \label{timeComplexity1}}
\end{figure}

\begin{figure}[t]
\centering
\begin{tikzpicture}
    \begin{axis}
        [
        ,width=8cm
        ,xlabel=\# Points
        ,ylabel=Time elapsed (sec)
        ,xtick=data,
       %,xtick={0,1,...,3}
        ,xticklabels={10k,20k,50k,100k, 200k}
        ,legend entries={1, 2, 3, 4, 5, 6}, legend style={at={(0.05,-0.25)},anchor=north west, draw=none, legend columns=-1}
        ]
        \addplot+[sharp plot] coordinates
        {(0,21.81) (1,63.6) (2,366.49) (3,1322.73) (4, 5501.24)};
        \addplot+[sharp plot, red] coordinates
        {(0,16.94) (1,44.62) (2,230.68) (3,785.68) (4, 3412.54)};
        \addplot+[sharp plot, green] coordinates
        {(0,16.42) (1,37.77) (2,186.69) (3,627.52) (4, 2451.26)};
        \addplot+[sharp plot, orange] coordinates
        {(0,15.74) (1,36.01) (2,173.83) (3,544.66) (4, 2012.89)};
        \addplot+[sharp plot, yellow] coordinates
        {(0,16.92) (1,33.17) (2,158.08) (3,480.03) (4, 1842.83)};
        \addplot+[sharp plot, purple] coordinates
        {(0,15.63) (1,33.84) (2,149.75) (3, 446.91) (4, 1708.76) };
    \end{axis}
\end{tikzpicture}
\caption{Evolution of time elapsed with a fixed number of workers. 48 CPUs per node.} \label{timeComplexity2}
\end{figure}

\subsection{Further Development and Improvement Proposals} \label{further_development}
The main development recommended would be performing a smart guess at execution time of the scale parameters. Empirically it has been proven that if the user is able to choose the right parameters, the algorithm is going to have a great performance. As a consequence, being able to guess them at execution time would guarantee a stability that for this moment can not be ensured.

Secondly, the DBSCAN method could be expanded to a generalized DBSCAN to detect arbitrary cluster densities removing the dependencies to the input parameters, \texttt{minPoints} and $\varepsilon$.

Lastly and in the opposite direction of the second proposal, some research into parallel optimization models so to guess the input parameters (rather than using a thumb rule) could be done.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%       ACKNOWLEDGEMENTS     %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section*{Acknowledgements}

Acks

% JC: In this section we will include the acknowledgements required. Don't worry about it by now. It will be just a paragraph with some references to projects and funding.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%       BIBLIOGRAPHY     %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{thebibliography}{1}
\bibitem{compss}
Badia, R. M., J. Conejero, C. Diaz, J. Ejarque, D. Lezzi, F. Lordan, C. Ramon-Cortes, and R. Sirvent \textit{COMP Superscalar, an interoperable programming framework} SoftwareX, Volumes 3–4, December 2015, Pages 32–36, 
\bibitem{related_1}
D. Arlia and M. Coppola, \textit{Experiments in Parallel Clustering
with DBSCAN} in Euro-Par 2001, Springer, LNCS, 2001, pp.
326-331.
\bibitem{related_2}
 S. Brecheisen et al., \textit{Parallel Density-Based Clustering of Complex Objects} Advances in Knowledge Discovery and
Data Mining, pp. 179-188, 2006.
\bibitem{related_3}
Yaobin He et al. \textit{MR-DBSCAN: a scalable MapReduce-based DBSCAN algorithm for heavily skewed data} Frontiers of Computer Science, vol 8, no. 1, pp 83-99, 2014.
\bibitem{related_4}
Md. Mostofa Ali Patwary et al. \textit{A new scalable parallel DBSCAN algorithm using the disjoint-set data structure} Conference: High Performance Computing, Networking, Storage and Analysis (SC), 2012 
\bibitem{related_5}
Alessandro Lulli, Matteo Dell'Amico, Pietro Michiardi, Laura Ricci \textit{NG-DBSCAN: scalable density-based clustering for arbitrary data}. Proceedings of the VLDB Endowment Volume 10 Issue 3, November 2016, p. 157-168 
\bibitem{ReferencePaper}
Ester, Martin Kriegel, Hans-Peter Sander, Jorg Xu, Xiaowei (1996). \textit{A density-based algorithm for discovering clusters in large spatial databases with noise.} Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96)
\bibitem{kmeans}
MacQueen, J. B. (1967). \textit{Some Methods for classification and Analysis of Multivariate Observations.} Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability.
\bibitem{pycompss}
Enric Tejedor, Yolanda Becerra, Guillem Alomar, Anna Queralt, Rosa M. Badia, Jordi Torres, Toni Cortes, Jesús Labarta, \textit{PyCOMPSs: Parallel computational workflows in Python}  IJHPCA 31(1): 66-82 (2017)
\bibitem{toolsDBSCAN}
J. Gonzalez, J. Gimenez, J. Labarta. \textit{Automatic detection of parallel applications computation phases}.
\end{thebibliography}


\end{document}


